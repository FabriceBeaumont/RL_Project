\input{Skript_praeambel.tex}

%% !TeX spellcheck = en_GB 

\begin{document}
\title{AUP - Theory}
\author{Fabrice Beaumont}
\maketitle
\tableofcontents

Included papers:
\begin{itemize}
	\item \cite{2018_Krakovna_penaslizing}
	\item 	
\end{itemize}

Papers to include:
\begin{itemize}
	\item \cite{2018_Krakovna_measuring}
	\item \cite{2020_Turner_avoiding}
	\item \cite{2020_Turner_conservative}
\end{itemize}

\chapter{Notation}

\begin{itemize}
	\item Arbitrary states $x$, $y$\\
	states at time step $t$: $s_t$
	\item Baseline state $s^\prime_t$\\
	Starting state baseline $s_0$, inaction baseline $s_t^{(0)}$, stepwise inaction baseline $s_t^{(t-1)}$
	\item Noop-action $a^{\text{noop}}$	
	\item $s_{m}^{(t)}$ - inaction rollout: State $t$ after $m-t$ no-operation actions ($m=t+k>t$). That is perform only no-operation actions after time-step $t$.
\end{itemize}

\chapter{Key notes}

\begin{itemize}
	\item $\Sigma$ - Set of states
	\item $A$ - Set of actions
\end{itemize}

\paragraph{Important definitions}:
\begin{itemize}
	\item $C(\tilde{s}, s) \in [0,1]$ - how easily can we obtain $s$ form $\tilde{s}$ (\textbf{coverage}, \textbf{reachability})
	\item Coverage matrix: $R=C\in \IR^{|\Sigma|\times|\Sigma|}$.\\
	(Written as $C$ or $R$ (if implemented as reachability).)
	\item $Q \in [0,1]^{|\Sigma|\times |A|}$ - \textbf{Q-table}\\
	$q_{s,a}$ - if in state $s$, how rewarding is it to perform action $a$
	\item $D \in \IR^{|\Sigma|} = \begin{pmatrix}
		d(S_t, S')
	\end{pmatrix}$ - deviation of state $S_t$ compared to the some baseline state $S'$
\end{itemize} 

\paragraph{Paper differences}: Updates of the \cite{2018_Krakovna_penaslizing} paper, compared to the old \cite{2018_Krakovna_measuring} paper:
\begin{itemize}
	\item Write $R$ instead of $C$
	\item Introduction of the \textbf{stepwise inaction baseline}
	\item Introduction of the scaling parameter beta $\beta$
	\item Scaling the computation of the $d_{\text{RR}}$ by the number of states
\end{itemize}

\chapter{Paper summaries}

Consider environments as discounted Markov Decision Processes (MDPs) $(S, \mathcal{A}, r, p, \gamma)$:
\begin{itemize}
	\item $S$ - set of states\\
	$s_t^\prime$ - baseline state at time $t$
	\item $\mathcal{A}$ - set of actions\\
	$a^{\text{noop}}\in \mathcal{A}$ - special no-operation action
	\item $r: S\times \mathcal{A} \to \IR$ - reward function
	\item $p: S\times S\times \mathcal{A} \to [0,1]$ - transition function
	\item $\gamma \in (0,1)$ - discount factor (sometimes written as $\gamma_r$ - reachability discount factor w.r.t. reward function $r$)
	\item $d: S\times \{ s_t^\prime \} \to \IR$ - deviation
	\item $\beta\in \IR$ - \textbf{deviation penalty} (\textbf{learned parameter})
\end{itemize}
At time step $t$, the agent receives the current state $s_t$, outputs the action $a_t$ drawn from its policy $\pi(a_t| s_t)$ and receives reward $r(s_t, a_t)$.

%TODO How to do text equations with linebreak
\begin{Definition}{Desirable properties of XZ}{}{} %TODO
	\begin{equation}\label{eq:Property1} \tag{Property 1}
		\text{Penalize the agent for effects on the environment if and only if those effects are unnecessary for achieving the objective.~\cite{2018_Krakovna_measuring}}
	\end{equation}
	
	\begin{equation}\label{eq:Property2} \tag{Property 2}
		\text{Distinguish between agent effects and environment events, and only penalize the agent for the former but not the latter.~\cite{2018_Krakovna_measuring}}
	\end{equation}
	
	\textbf{Sensitivity to the reversible-ness of the agent's effects}:
	\begin{equation}\label{eq:Property3} \tag{Property 3}
		\text{Give a higher penalty for irreversible effects than for reversible effects.~\cite{2018_Krakovna_measuring}}
	\end{equation}
	
	\textbf{Sensitivity to the magnitude of the agentâ€™s irreversible effects}:
	\begin{equation}\label{eq:Property4} \tag{Property 4}
		\text{(Cumulative penalty) The penalty should accumulate when more irreversible effects occur.~\cite{2018_Krakovna_measuring}}		
	\end{equation}
	For example, if the agent starts in state $S_0$, takes an irreversible action that leads to state $S_1$, and then takes another irreversible action that leads to state $S_2$, then $d(S_2; S_0) > d(S_1;S_0)$.
\end{Definition}

\begin{Definition}{Baselines}{}{}
	We define different baselines in order to compare the actions of an agent at any state with these baselines.
	
	\textbf{Starting state}: Use the state of the environment at $t=0$.
	
	\textbf{Inaction baseline}: Simulate the environment as if the agent never spawned. (That is it performs the $a^{\text{noop}}$ at every time-step.)
	
	\textbf{Stepwise-inaction baseline}: Simulate the environment as if the agent has done noting, instead of the last chosen action.
	
	
	It may be useful to not (just) compare the current state and   baseline state of the current time-step \textbf{inaction-rollout}.
\end{Definition}

%					0	1	2	3	4	5	6	7	8	9	10	
%starting state bl	*				
%inaction baseline	-	-	-	-	-	-	-	-	-	-	-
%sw inaction bl		a1	a2	a3	a4	a5	- 	-	-	-	-	-
%agents actions		a1	a2	a3	a4	a5	a6  -	-	-	-	-
%
%State $t=5$, inaction rollout until state 10 ($k=5$).

\begin{Definition}{Intrinsic pseudo-reward}{}{}
	By adding a \textbf{penalty for side effects}\footnote{Side effects are impacts to the environment, which are not necessary to complete the main task} to the reward function we can implement an intrinsic pseudo-reward.
	Therefore, we subtract at time $t$ an impact penalty, which is a scaled deviation penalty from the deviation of the current state from the baseline state $s_t^\prime$.
	\[  r_\beta(s_t, a_t) := r(s_t, a_t) - \beta \; \cdot \; d(s_{t+1}, s_{t+1}^\prime ) \]
\end{Definition}


\begin{Definition}{Inaction rollout}{}{}
	An \textbf{inaction rollout} from state $s_t$ is a sequence of states obtained by following the inaction policy ($a^{\text{noop}}$) starting from that state. Thus state $s_{t+2}^{(t)}$ denotes the state at time step $t+2$, after arriving at state $s_t$ at time step $t$ and performing the no-operation action for two time steps.
	
	This allows for an easy comparison to environment state after choosing the no-operation action at every time step starting from the baseline state: $s^{\prime(t)}_{t+2}$
\end{Definition}


\begin{Definition}{Deviation measure}{}{}
	\dots
	
	is called \textbf{symmetric}, if \dots
\end{Definition}
Using a symmetric deviation measure implies, that all actions are reversible.

We define an asymmetric deviation measure called \textbf{reachability}.


\begin{Definition}{Reachability}{}{}
	We define a \textbf{reachability} $R:S\times S\to[0,1]$ as a measure of difficulty to get from state $x$ to state $y$ ($x\neq y$).
	We use the parameter $\gamma_r\in(0,1]$ to define the importance of time. For low $\gamma$, it is expensive to need more time-steps. For high $\gamma$, it is cheaper to need more time-steps. The special case $\gamma=1$, where times does not matter is discussed below.
	
	\[ R(x,y) := \max_\pi \gamma^{N_{\pi}(x, y)} \qquad \Big(= \max_\pi \IE\big[ \gamma^{N_{\pi}(x, y)]} \big] \Big)\]
	(Use the $\IE$ notation only for the proof of the statement below for undiscounted reachability.)
	
	A recursive computation can be done like this:
	\begin{flalign*}
		R(x,y) &:= \gamma \max\limits_{a} \sum_{z \in S} p(z| \; x,a) R(z,y)\\
		&= \gamma^{n} \max\limits_{a_1} \sum_{z_1 \in S} p(z_1| \; x,a_1) \Big(\max\limits_{a_2}\sum_{z_2 \in S} p(z_2| \; z_1,a_2) \dots \\
		&&&\hspace{-1cm}\max\limits_{a_{n}} \sum_{z_{n} \in S} (0+ p(y| \; z_{n},a_{n})*1)\Big)
	\end{flalign*}
	where $n=N_{\pi}(x,y)$. And it is $R(y,y)=1$.\\
	
	\textit{Special case}: \textbf{Undiscounted reachability} ($\gamma=1$), which computes whether $y$ is reachable in any number of steps. In this cased it is (see paper for proof):
	\[ R(x,y) = \max_\pi \IP\big( N_{\pi}(x,y)<\infty \big)  = \begin{cases}
		1 \ y \text{if is reachable from }x\\
		0 \ \text{otherwise}
	\end{cases}\]	
	
	The \textbf{unreachability} (UR) \textbf{deviation measure} $d_{\text{UR}}:S\times S\to[0,1]$ is then defined as:
	\[ d_{\text{UR}}(x,y) := 1-R(x,y) \]
	$d_{\text{UR}}(x,y)$ close to $1$ means low reachability, high unreachability.
	
	\textit{Note}: The undiscounted unreachability measure only penalizes irreversible transitions\footnote{$d_{\text{UR}}(x,y)=1$ if unreachable, $0$ else.}, while the
	discounted measure also penalizes reversible transitions.
\end{Definition}

The unreachability deviation measure is often used to compute the unreachability to the baseline state $s^\prime_t$ from a state $s_t$: $d_{\text{UR}}(s_t,s^\prime_t)$.


\begin{Definition}{Relative reachability}{}{}
	The \textbf{relative reachability} (\textbf{RR}) measure $d_{\text{RR}}: S\times S\to [0,1]$ is the average reduction in reachability of all states $s$ from the current state $s_t$ compared to the baseline $s^\prime_t$:
	
	\[ d_{\text{RR}}(x,y) := \frac{1}{|S|} \sum_{s\in S} \max\big( R(s^\prime_t, s)-R(s_t,s),\; 0 \big) \]
	
\end{Definition}

\subsection{Generalization}

The RR (and AU% TODO
) deviation measures are examples of the so called \textit{value-difference measures}:

\begin{Definition}{State value measure}{}{}
	The \textbf{state-value measure} $V_v:S\to \IR$ denotes the value of a state $x$. 
	Let $\mathcal{V}$ be a set of value sources and $v\in V$.
	$V_v$ is defined as the maximum sum of all value functions for all states, which are reachable from the state in question $x$. 
	To express this reachability let $x_t^{\pi}$ denote the state obtained from $x$ by following policy $\pi$ for $t$ steps. It is
	\[ V_v(x) := \max_\pi \sum_{t=0}^{\infty} \gamma_v^t \ v(x_t^{\pi}) \]
	
	For the stepwise inaction baseline, the definition is extended to a \textbf{rollout value measure} $RV_v:S\to\IR$. Recall that for a state $x_t$ its rollout of $k$ time steps, starting from time step $t$ is denoted as $x_{t+k}^{(t)}$. It is:
	\[ RV_v(x_t) := (1-\gamma_v) \sum_{k=0}^{\infty} \gamma_v^k V_v(x^{(t)}_{t+k})  \]
	This rollout value measure can be computed recursively as well:
	\[ RV_v(x_t) = (1-\gamma_v) \big( V_v(x_t) + \gamma_v RV_v(I(x_t))\big)\]
	where $I(x_t)$ is the inaction function that gives the state reached by following the inaction policy form state $x_t$.
\end{Definition}
To better understand the definition of the $RV$ function, lets unravel it:
\[ RV_v(x_t) := (1-\gamma_v) \sum_{k=0}^{\infty} \gamma_v^k V_v(x^{(t)}_{t+k}) 
= \sum_{k=0}^{\infty} \gamma_v^k V_v(x^{(t)}_{t+k}) \ - \ \gamma_v \sum_{k=0}^{\infty} \gamma_v^k V_v(x^{(t)}_{t+k}) \]
and 
\[ RV_v(x_t) := (1-\gamma_v) \sum_{k=0}^{\infty} \gamma_v^k V_v(x^{(t)}_{t+k}) = (1-\gamma_v) \sum_{k=0}^{\infty} \gamma_v^k
\max_\pi \sum_{t=0}^{\infty} \gamma_v^t \ v(x_t^{\pi}) \]

Examples:
\begin{itemize}
	\item RR: $v=\tilde{s}$ reachability function as comparison to another state $\tilde{s}$ and $\mathcal{V}=\mathcal{S}$ the set of all states.
	Note that in this case $\gamma_v$ can be written as $\gamma$ since it is constant for all states.\\
	In the definition of $v$ as a function, the value of a state is defined recursively as the reachability of other states $s$ from it:
	\begin{flalign*}
		V_{\tilde{s}}(x) &:= \max_\pi \sum_{t=0}^{\infty} \gamma^t \ v(x_t^{\pi})\\
		&= R(x,\tilde{s})\\
		&= \gamma \max\limits_{a} \sum_{z \in S} p(z| \; x,a) R(z,\tilde{s})
	\end{flalign*}
	with $R(x,x)=1$.\\
	The equivalent recursive formula goes as follows:
	\begin{flalign*}
		RV_{\tilde{s}}(x_t) &:= (1-\gamma_v) \big( V_{\tilde{s}}(x_t) + \gamma_v RV_{\tilde{s}}(I(x_t))\big)\\
		&= RV(x_t, \tilde{s})\\		
		&= (1-\gamma) \big( R(x_t, \tilde{s}) + \gamma RV_{\tilde{s}}(I(x_t))\big)\\
		&= (1-\gamma) \big( \gamma \max\limits_{a} \sum_{z \in S} p(z| \; x_t,a) R(z,\tilde{s}) + \gamma RV_{\tilde{s}}(I(x_t))\big)\\
		&= (1-\gamma) \big( \gamma \max\limits_{a} \sum_{z \in S} p(z| \; x_t,a) R(z,\tilde{s}) + \gamma^2 \sum_{z \in S} p(z| \; x_t,a^{\text{noop}}) R(z,\tilde{s}) \big)
	\end{flalign*}
	\item AU: $v=r$ a reward function and $\mathcal{V}=\mathcal{R}$ a collection of reward functions.\\
	\[ V_r(x) := \max_\pi \sum_{t=0}^{\infty} \gamma_r^t \ r(x_t^{\pi}) \]
	The reward of one state is computed by adding up all rewards from the states reached while transitioning to the target state. That is
	$v(x_t^{\pi}):=r(x_t^{\pi}) = \sum_{t_1=0}^{\infty} r(x_{t_1}, a_{t_1})$ such that $(x_{t_1}, a_{t_1})\in\pi$. 
\end{itemize}

\begin{Definition}{Value-difference measure}{}{}
	The \textbf{value-difference measure} $d_{\text{VD}}:S\times S\to \IR$ denotes the average gain in reward by obtaining a state $x$ compared to a state $y$. Let $\mathcal{V}$ be a set of value sources, $V_v$ be a state-value function for reward $v\in\mathcal{V}$. Let $w_v\in\IR$ be a weighting or normalizing factor (usually $w_v:=\frac{1}{|\mathcal{V}|}$)and $f:\IR\to\IR$ a summarizing function.
	(Examples are given below.)
	
	\[ d_{VD}(s_t;\; s_t^\prime) := \sum_{v\in \mathcal{V}} w_v f\big(V_v(s_t^\prime)-V_v(s_t)\big) \]
	
	For the stepwise inaction baseline, the definition is extended to rollout states for the current state $s_t$, and the baseline state $s^\prime := s_t^{(t-1)}$ (stepwise inaction baseline). 
\end{Definition}


\section{Formalizations of Implementations}

\subsection{Version 28.07.2023}

Mapping of code names to mathematical variables:
\begin{multicols}{2}
\begin{itemize}
	%\item \texttt{c\_delta} $=\delta_c$
	\item \texttt{q\_discount} $=\gamma$\\
	(coverage discount factor)
	\item \texttt{c\_table} $=C$
	\item \texttt{learning\_rate} $=\alpha$
	\item \texttt{d\_rr} $=d_{\text{rr}}$
	\item \texttt{rr\_reward} $=R^\prime$
	\item \texttt{timestep.reward} $=R$
	\item \texttt{beta} $=\beta$
	\item \texttt{q\_table} $=Q$
	\item \texttt{max\_action} (with \texttt{state\_id} $s$) \\$a^*=\arg\max\limits_{a\in A} Q[s_{t},a]$	
	\item \texttt{state\_old} $=s_{t}$
	\item \texttt{state\_new} $=s_{t+1}$
	\item \texttt{state\_baseline} $=s^\prime_{t}$
\end{itemize}	
\end{multicols}

\paragraph{Q-Learning:}
\begin{flalign*}	
	Q_{s_{t}, a_{t}} &= Q_{s_{t}, a_{t}} + \alpha * \underbrace{
		\big( R + d_q * Q_{s_{t+1}, a^*} -Q_{s_{t}, a_t} \big)
	}_{\texttt{q\_delta}} &&&\textbf{Q-table update}
\end{flalign*}

\paragraph{RR-Learning:}
\begin{flalign*}
	C_{s_{t},*} &= C_{s_{t},*} + \alpha * \big( \underbrace{ \gamma C_{*,s_{t}} - C_{s_{t+1},*} + \delta_{s_{t}, s_{t+1}} }_{\texttt{c\_delta}}\big) &&&\textbf{Coverage update}\\
	R^\prime &= R - \beta \underbrace{\frac{1}{|S|} \sum_{s\in S} max\big( C_{s^\prime, s}-C_{s_{t}, s}, 0\big) }_{d_{\text{rr}}} &&&\textbf{Reward adjustment}\\
	Q_{s_{t}, a_{t}} &= Q_{s_{t}, a_{t}} + \alpha * \underbrace{
		\big( R^\prime + d_q * Q_{s_{t+1}, a^*} -Q_{s_{t}, a_t} \big)
	}_{\texttt{q\_delta}} &&&\textbf{Q-table update}
\end{flalign*}

Let give the coverage update (relative reachability update) another try. In the paper \cite{2018_Krakovna_penaslizing} the reachability of state $s_{t+1}$ from state $s_{t}$ is defined recursively as $C_{s, s} = 1$ and for $s_{t+1}\neq s_{t}$:
\begin{flalign*} 
	C_{s_{t}, s_{t+1}} &= \gamma \max_a \sum_{s\in S} p(s| s_{t}, a) C_{s, s_{t+1}} \\
	&= \gamma \max_a \sum_{s\in S} C_{s, s_{t+1}} &&\text{Deterministic state transitions}\\	
\end{flalign*}
Thus now the update depends on the set of states, which can be used to reach the state $s_{t+1}$ by taking any one single action as well. 
Thus for the computation, perform each action in every state $s$ and save the ones, that lead to a transition to state $s_{t+1}$. Take the maximum row sum from each of these states and weight them by the coverage discount factor.

Examples:
\begin{itemize}
	\item RR: $w_v = \frac{1}{|\mathcal{S}|}$ and \\
	$f(d) = \max(d,0)$ (\enquote{truncated difference}, penalizing decreases in value)
	\item AU: $w_v = \frac{1}{|\mathcal{R}|}$ and \\
	$f(d) = |d|$ (\enquote{absolute difference}, penalizing all changes in value)
\end{itemize}

\newpage
%%%% ----------------------- %%%%
%%%% ------Bibliography----- %%%%
%%%% ----------------------- %%%%
\bibliographystyle{plain}
\bibliography{./bibliography}

\end{document}


