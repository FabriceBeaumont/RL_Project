\input{Skript_praeambel.tex}

%% !TeX spellcheck = en_GB 

\begin{document}
\title{AUP - Theory}
\author{Fabrice Beaumont}
\maketitle
\tableofcontents

Included papers:
\begin{itemize}
	\item \cite{2018_Krakovna_penaslizing}
	\item 	
\end{itemize}

Papers to include:
\begin{itemize}
	\item \cite{2018_Krakovna_measuring}
	\item \cite{2020_Turner_avoiding}
	\item \cite{2020_Turner_conservative}
\end{itemize}

\chapter{Notation}

\begin{itemize}
	\item Arbitrary states $x$, $y$\\
	states at time step $t$: $s_t$
	\item Baseline state $s^\prime_t$\\
	Starting state baseline $s_0$, inaction baseline $s_t^{(0)}$, stepwise inaction baseline $s_t^{(t-1)}$
	\item Noop-action $a^{\text{noop}}$	
	\item $s_{m}^{(t)}$ - inaction rollout: State $t$ after $m-t$ no-operation actions ($m=t+k>t$). That is perform only no-operation actions after time-step $t$.
\end{itemize}

\chapter{Key notes}

\begin{itemize}
	\item $\Sigma$ - Set of states
	\item $A$ - Set of actions
\end{itemize}

\paragraph{Important definitions}:
\begin{itemize}
	\item $C(\tilde{s}, s) \in [0,1]$ - how easily can we obtain $s$ form $\tilde{s}$ (\textbf{coverage}, \textbf{reachability})
	\item Coverage matrix: $R=C\in \IR^{|\Sigma|\times|\Sigma|}$.\\
	(Written as $C$ or $R$ (if implemented as reachability).)
	\item $Q \in [0,1]^{|\Sigma|\times |A|}$ - \textbf{Q-table}\\
	$q_{s,a}$ - if in state $s$, how rewarding is it to perform action $a$
	\item $D \in \IR^{|\Sigma|} = \begin{pmatrix}
		d(S_t, S')
	\end{pmatrix}$ - deviation of state $S_t$ compared to the some baseline state $S'$
\end{itemize} 

\paragraph{Paper differences}: Updates of the \cite{2018_Krakovna_penaslizing} paper, compared to the old \cite{2018_Krakovna_measuring} paper:
\begin{itemize}
	\item Write $R$ instead of $C$
	\item Introduction of the \textbf{stepwise inaction baseline}
	\item Introduction of the scaling parameter beta $\beta$
	\item Scaling the computation of the $d_{\text{RR}}$ by the number of states
\end{itemize}

\chapter{Paper summaries}

Consider environments as discounted Markov Decision Processes (MDPs) $(S, \mathcal{A}, r, p, \gamma)$:
\begin{itemize}
	\item $S$ - set of states\\
	$s_t^\prime$ - baseline state at time $t$
	\item $\mathcal{A}$ - set of actions\\
	$a^{\text{noop}}\in \mathcal{A}$ - special no-operation action
	\item $r: S\times \mathcal{A} \to \IR$ - reward function
	\item $p: S\times S\times \mathcal{A} \to [0,1]$ - transition function
	\item $\gamma \in (0,1)$ - discount factor (sometimes written as $\gamma_r$ - reachability discount factor w.r.t. reward function $r$)
	\item $d: S\times \{ s_t^\prime \} \to \IR$ - deviation
	\item $\beta\in \IR$ - \textbf{deviation penalty} (\textbf{learned parameter})
\end{itemize}
At time step $t$, the agent receives the current state $s_t$, outputs the action $a_t$ drawn from its policy $\pi(a_t| s_t)$ and receives reward $r(s_t, a_t)$.

%TODO How to do text equations with linebreak
\begin{Definition}{Desirable properties of XZ}{}{} %TODO
	\begin{equation}\label{eq:Property1} \tag{Property 1}
		\text{Penalize the agent for effects on the environment if and only if those effects are unnecessary for achieving the objective.~\cite{2018_Krakovna_measuring}}
	\end{equation}
	
	\begin{equation}\label{eq:Property2} \tag{Property 2}
		\text{Distinguish between agent effects and environment events, and only penalize the agent for the former but not the latter.~\cite{2018_Krakovna_measuring}}
	\end{equation}
	
	\textbf{Sensitivity to the reversible-ness of the agent's effects}:
	\begin{equation}\label{eq:Property3} \tag{Property 3}
		\text{Give a higher penalty for irreversible effects than for reversible effects.~\cite{2018_Krakovna_measuring}}
	\end{equation}
	
	\textbf{Sensitivity to the magnitude of the agentâ€™s irreversible effects}:
	\begin{equation}\label{eq:Property4} \tag{Property 4}
		\text{(Cumulative penalty) The penalty should accumulate when more irreversible effects occur.~\cite{2018_Krakovna_measuring}}		
	\end{equation}
	For example, if the agent starts in state $S_0$, takes an irreversible action that leads to state $S_1$, and then takes another irreversible action that leads to state $S_2$, then $d(S_2; S_0) > d(S_1;S_0)$.
\end{Definition}

\begin{Definition}{Baselines}{}{}
	We define different baselines in order to compare the actions of an agent at any state with these baselines.
	
	\textbf{Starting state}: Use the state of the environment at $t=0$.
	
	\textbf{Inaction baseline}: Simulate the environment as if the agent never spawned. (That is it performs the $a^{\text{noop}}$ at every time-step.)
	
	\textbf{Stepwise-inaction baseline}: Simulate the environment as if the agent has done noting, instead of the last chosen action.
	
	
	It may be useful to not (just) compare the current state and   baseline state of the current time-step \textbf{inaction-rollout}.
\end{Definition}

%					0	1	2	3	4	5	6	7	8	9	10	
%starting state bl	*				
%inaction baseline	-	-	-	-	-	-	-	-	-	-	-
%sw inaction bl		a1	a2	a3	a4	a5	- 	-	-	-	-	-
%agents actions		a1	a2	a3	a4	a5	a6  -	-	-	-	-
%
%State $t=5$, inaction rollout until state 10 ($k=5$).

\begin{Definition}{Intrinsic pseudo-reward}{}{}
	By adding a \textbf{penalty for side effects}\footnote{Side effects are impacts to the environment, which are not necessary to complete the main task} to the reward function we can implement an intrinsic pseudo-reward.
	Therefore, we subtract at time $t$ an impact penalty, which is a scaled deviation penalty from the deviation of the current state from the baseline state $s_t^\prime$.
	\[  r_\beta(s_t, a_t) := r(s_t, a_t) - \beta \; \cdot \; d(s_{t+1}, s_{t+1}^\prime ) \]
\end{Definition}


\begin{Definition}{Inaction rollout}{}{}
	An \textbf{inaction rollout} from state $s_t$ is a sequence of states obtained by following the inaction policy ($a^{\text{noop}}$) starting from that state. Thus state $s_{t+2}^{(t)}$ denotes the state at time step $t+2$, after arriving at state $s_t$ at time step $t$ and performing the no-operation action for two time steps.
	
	This allows for an easy comparison to environment state after choosing the no-operation action at every time step starting from the baseline state: $s^{\prime(t)}_{t+2}$
\end{Definition}


\begin{Definition}{Deviation measure}{}{}
	\dots
	
	is called \textbf{symmetric}, if \dots
\end{Definition}
Using a symmetric deviation measure implies, that all actions are reversible.

We define an asymmetric deviation measure called \textbf{reachability}.


\begin{Definition}{Reachability}{}{}
	We define a \textbf{reachability} $R:S\times S\to[0,1]$ as a measure of difficulty to get from state $x$ to state $y$ ($x\neq y$).
	We use the parameter $\gamma_r\in(0,1]$ to define the importance of time. For low $\gamma$, it is expensive to need more time-steps. For high $\gamma$, it is cheaper to need more time-steps. The special case $\gamma=1$, where times does not matter is discussed below.
	
	\[ R(x,y) := \max_\pi \gamma^{N_{\pi}(x, y)} \qquad \Big(= \max_\pi \IE\big[ \gamma^{N_{\pi}(x, y)]} \big] \Big)\]
	(Use the $\IE$ notation only for the proof of the statement below for undiscounted reachability.)
	
	A recursive computation can be done like this:
	\begin{flalign*}
		R(x,y) &:= \gamma_r \max\limits_{a} \sum_{z \in S} p(z| \; x,a) R(z,y)\\
		&\stackrel{?}{=} \gamma_r^{n} \max\limits_{a_1} \sum_{z_1 \in S} p(z_1| \; x,a_1) \Big( \max\limits_{a_2} \sum_{z_2 \in S} p(z_2| \; z_1,a_2) \dots \max\limits_{a_{n}} \sum_{z_{n} \in S} (0+ p(y| \; z_{n},a_{n})*1)\Big)
	\end{flalign*}
	where $n=N_{\pi}(x,y)$. And it is $R(y,y)=1$.\\
	
	\textit{Special case}: \textbf{Undiscounted reachability} ($\gamma_r=1$), which computes whether $y$ is reachable in any number of steps. In this cased it is (see paper for proof):
	\[ R(x,y) = \max_\pi \IP\big( N_{\pi}(x,y)<\infty \big)  = \begin{cases}
		1 \ y \text{ is reachable from }x\\
		0 \ \text{otherwise}
	\end{cases}\]	
	
	The \textbf{unreachability} (UR) \textbf{deviation measure} $d_{\text{UR}}:S\times S\to[0,1]$ is then defined as:
	\[ d_{\text{UR}}(x,y) := 1-R(x,y) \]
	$d_{\text{UR}}(x,y)$ close to $1$ means low reachability, high unreachability.
	
	\textit{Note}: The undiscounted unreachability measure only penalizes irreversible transitions\footnote{$d_{\text{UR}}(x,y)=1$ if unreachable, $0$ else.}, while the
	discounted measure also penalizes reversible transitions.
\end{Definition}

The unreachability deviation measure is often used to compute the unreachability to the baseline state $s^\prime_t$ from a state $s_t$: $d_{\text{UR}}(s_t,s^\prime_t)$.


\begin{Definition}{Relative reachability}{}{}
	The \textbf{relative reachability} (\textbf{RR}) measure $d_{\text{RR}}: S\times S\to [0,1]$ is the average reduction in reachability of all states $s$ from the current state $s_t$ compared to the baseline $s^\prime_t$:
	
	\[ d_{\text{RR}}(x,y) := \frac{1}{|S|} \sum_{s\in S} \max\big( R(s^\prime_t, s)-R(s_t,s),\; 0 \big) \]
	
\end{Definition}

\begin{Definition}{Attainable utility}{}{}
	The \textbf{attainable utility measure} $d_{\text{VD}}:S\times S\to \IR$ denotes the average gain in reward by obtaining a state $x$ compared to a state $y$. To define it, we define the \textbf{value} $V_r:S\to \IR$ of a state $x$ according to a reward function $r$. Therefore let $x_t^{\pi}$ denote the state obtained from $x$ by following policy $\pi$ for $t$ steps. It is
	\[ V_r(x) := \max_\pi \sum_{t=0}^{\infty} \gamma_r^k \ r(x_t^{\pi}) \]
	
	\texttodo{Was ist $k$? $x$ is die reward function $r$? Die ist aber fuer States UND Aktionen definiert. Ist die Summe aller rewards fuer alle States und Aktionen gemaess $\pi$ gemeint?]} 
	$r(x_t^{\pi}) = \sum_{t_1=0}^{\infty} r(x_{t_1}, a_{t_1})$ sodass $(x_{t_1}, a_{t_1})\in\pi$. 
	\texttodo{Vermutung fuer $k$: Wie beim inaction rollout time step difference zum aktuellen time step. Frage: Sollte das values eines states nicht abhaengig vom time step des states sein?}
\end{Definition}


\newpage
%%%% ----------------------- %%%%
%%%% ------Bibliography----- %%%%
%%%% ----------------------- %%%%
\bibliographystyle{plain}
\bibliography{./bibliography}

\end{document}


